{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identified 113062 images\n"
     ]
    }
   ],
   "source": [
    "file_locations = glob('./Captchas/*')\n",
    "captcha_names = [file.split('/')[-1].split('.')[0] for file in file_locations]\n",
    "print( f'identified {len(file_locations)} images' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 unique characters: {'6': 0, 'F': 1, '8': 2, 'w': 3, 'n': 4, 'C': 5, 's': 6, '2': 7, 'P': 8, 't': 9, 'S': 10, '7': 11, 'N': 12, 'T': 13, 'a': 14, 'u': 15, 'y': 16, 'e': 17, 'g': 18, 'b': 19, 'R': 20, 'Y': 21, 'v': 22, 'O': 23, 'B': 24, 'Q': 25, 'D': 26, 'E': 27, '5': 28, 'm': 29, 'd': 30, 'W': 31, 'j': 32, 'r': 33, 'x': 34, 'q': 35, 'K': 36, '3': 37, 'L': 38, 'A': 39, 'Z': 40, 'H': 41, '9': 42, 'i': 43, '1': 44, '4': 45, 'h': 46, 'U': 47, 'z': 48, 'M': 49, 'I': 50, 'J': 51, 'p': 52, 'f': 53, 'l': 54, 'X': 55, 'c': 56, 'G': 57, 'V': 58, 'k': 59}\n"
     ]
    }
   ],
   "source": [
    "# Unique characters is global -- expect only 60 out\n",
    "unique_characters = set(char for name in captcha_names for char in name)\n",
    "\n",
    "character_encode = {char:i for i,char in enumerate(unique_characters)}\n",
    "\n",
    "print( f'{len(unique_characters)} unique characters: {character_encode}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Captcha_Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = data\n",
    "        self.y = torch.tensor(labels)\n",
    "\n",
    "    @classmethod\n",
    "    def import_image(cls, location:str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Import a single image.\n",
    "\n",
    "        Parameters: location (str) Location of image\n",
    "        Returns: (np.ndarray) Image dimensions = Captchas 40 x 150 x 3 RGB channels\n",
    "        \"\"\"\n",
    "        image = Image.open(location)\n",
    "        image.load()\n",
    "        #image.show()\n",
    "        data = np.asarray(image, dtype='float32')\n",
    "        return data\n",
    "    @classmethod\n",
    "    def stack_images(cls, file_locations:List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Stack imageset from directory.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (np.ndarray) len(file_locations) x image dimensions\n",
    "        \"\"\"\n",
    "        return np.array([cls.import_image(location) for location in file_locations ])\n",
    "    \n",
    "    @classmethod\n",
    "    def read_label_names(cls, file_locations:List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Simply extracts labels from filenames.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (List[str]) List of label names\n",
    "        \"\"\"\n",
    "        labels = [file.split('/')[-1].split('.')[0] for file in file_locations]\n",
    "        \n",
    "        return [[character_encode[char] for char in name] for name in labels]\n",
    "\n",
    "\n",
    "        # print(labels)\n",
    "        # return [[character_encode[char] for char in name] for name in labels]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_dir(cls, file_locations:List[str]):\n",
    "        \"\"\"\n",
    "        Instantiate from only a list of files.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (Captcha_Dataset) object\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            cls.stack_images(file_locations),\n",
    "            cls.read_label_names(file_locations)\n",
    "        )\n",
    "\n",
    "    def transform(self, image:np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Dataset transform for loading.\"\"\"\n",
    "        return T.ToTensor()(image) # This is a hack for now.\n",
    "        # Not sure why, but this transforming doesn't work. It's weird. Idk.\n",
    "        # I originally tried using only PIL images and then resizing from there, but it didn't work.\n",
    "        # Tried now going from PIL --> ndarray --> PIL --> Tensor; also doesn't work. \n",
    "        # Bit lost.\n",
    "        # return  T.Compose([\n",
    "        #     T.ToPILImage(),\n",
    "        #     T.Resize([40, 150]),\n",
    "        #     T.ToTensor()\n",
    "        #     ])(image)\n",
    "\n",
    "    def __getitem__(self, index:int) -> Tuple[torch.Tensor, str]:\n",
    "        \"\"\"Select one sample. DataLoader accesses samples through this function.\"\"\"\n",
    "        return self.transform(self.X[index]), self.y[index]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Also needed for DataLoader.\"\"\"\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 40, 150, 3)\n",
      "Sample of first 128 images in format 40px x 150px x 3 RGB channels, of type <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "sample = Captcha_Dataset.from_dir(file_locations[0:128])\n",
    "\n",
    "print(f'{sample.X.shape}\\nSample of first 128 images in format 40px x 150px x 3 RGB channels, of type {type(sample.X[0][0][0][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(sample, \\\n",
    "    4, # Fetch 4 samples per batch\n",
    "    shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each batch has a dataset of shape torch.Size([4, 3, 40, 150]) and a corresponding set of 4 labels.\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(dl)\n",
    "images, labels = next(dataiter)\n",
    "print(f'Each batch has a dataset of shape {images.shape} and a corresponding set of {len(labels)} labels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, # 3 input channels\n",
    "            6, # 6 output channels\n",
    "            5) # kernel of size 5x5\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, # kernel size of 2\n",
    "            2) # stride of 2\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 in / 16 out / 5x5 kernel\n",
    "\n",
    "        self.fc1 = nn.Linear(3808, # features in\n",
    "            1404) # features out\n",
    "\n",
    "        self.fc2 = nn.Linear(1404, 702)\n",
    "        self.fc3 = nn.Linear(702, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)#nn.Softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "[1,     1] loss: -462624248561.664\n",
      "[1,     2] loss: -1137976516018.176\n",
      "[1,     3] loss: -1615332871503.872\n",
      "[1,     4] loss: -2139805118889.984\n",
      "[1,     5] loss: -3301313928495.104\n",
      "[1,     6] loss: -3859321246973.952\n",
      "[1,     7] loss: -4524389013913.600\n",
      "[1,     8] loss: -5234046057578.496\n",
      "[1,     9] loss: -5743536687284.224\n",
      "[1,    10] loss: -6573962641276.928\n",
      "[1,    11] loss: -7125715984056.320\n",
      "[1,    12] loss: -7687504283041.792\n",
      "[1,    13] loss: -8154613517647.872\n",
      "[1,    14] loss: -8976243915489.279\n",
      "[1,    15] loss: -9761567277056.000\n",
      "[1,    16] loss: -10606484182269.951\n",
      "[1,    17] loss: -11599641816596.480\n",
      "[1,    18] loss: -12349139647987.713\n",
      "[1,    19] loss: -12954745203326.977\n",
      "[1,    20] loss: -14022971752448.000\n",
      "[1,    21] loss: -14917231657877.504\n",
      "[1,    22] loss: -15356013637009.408\n",
      "[1,    23] loss: -15982385157898.240\n",
      "[1,    24] loss: -16685439024889.855\n",
      "[1,    25] loss: -17763585337851.902\n",
      "[1,    26] loss: -18658125825441.793\n",
      "[1,    27] loss: -19673133434798.078\n",
      "[1,    28] loss: -20879421439213.566\n",
      "[1,    29] loss: -22224611370336.258\n",
      "[1,    30] loss: -22871921796317.184\n",
      "[1,    31] loss: -24274528997736.449\n",
      "[1,    32] loss: -25587391246893.055\n",
      "Starting epoch 1\n",
      "[2,     1] loss: -980399098429.440\n",
      "[2,     2] loss: -2213051491155.968\n",
      "[2,     3] loss: -3474262195175.424\n",
      "[2,     4] loss: -4342200661966.848\n",
      "[2,     5] loss: -5745552503341.056\n",
      "[2,     6] loss: -6428289967063.040\n",
      "[2,     7] loss: -7741450582228.992\n",
      "[2,     8] loss: -9088647873691.648\n",
      "[2,     9] loss: -10490296626315.264\n",
      "[2,    10] loss: -12199692133728.256\n",
      "[2,    11] loss: -13577391880273.920\n",
      "[2,    12] loss: -15242548285014.016\n",
      "[2,    13] loss: -16762140195028.992\n",
      "[2,    14] loss: -18306457158025.215\n",
      "[2,    15] loss: -19639359993020.414\n",
      "[2,    16] loss: -20867541995880.449\n",
      "[2,    17] loss: -22650888384413.695\n",
      "[2,    18] loss: -24367341865271.297\n",
      "[2,    19] loss: -25929723729149.953\n",
      "[2,    20] loss: -27345458766544.895\n",
      "[2,    21] loss: -29023692541394.945\n",
      "[2,    22] loss: -30950761152118.785\n",
      "[2,    23] loss: -32562641306124.289\n",
      "[2,    24] loss: -33831339614535.680\n",
      "[2,    25] loss: -35668747964383.234\n",
      "[2,    26] loss: -37216013959299.070\n",
      "[2,    27] loss: -39246873541738.492\n",
      "[2,    28] loss: -40726786396389.375\n",
      "[2,    29] loss: -43038053790384.125\n",
      "[2,    30] loss: -44748137163653.117\n",
      "[2,    31] loss: -47086457214468.094\n",
      "[2,    32] loss: -49477769377087.484\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    print(f'Starting epoch {epoch}')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dl, 0):\n",
    "        # print(f'Iteration {i}')\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # print('Read data.')\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print('Grads zeroed.')\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.1803e+09, 7.8717e+09, 8.2863e+09, 7.6505e+09, 8.4072e+09],\n",
      "        [5.5434e+09, 5.3342e+09, 5.6152e+09, 5.1844e+09, 5.6971e+09],\n",
      "        [8.2874e+09, 7.9747e+09, 8.3947e+09, 7.7507e+09, 8.5172e+09],\n",
      "        [1.3922e+09, 1.3397e+09, 1.4103e+09, 1.3021e+09, 1.4309e+09]])\n",
      "tensor([[32, 55,  6, 38,  5],\n",
      "        [22, 37, 40, 31, 41],\n",
      "        [14,  8, 42, 56, 22],\n",
      "        [56, 46, 55, 45, 45]])\n",
      "tensor([[8.4344e+09, 8.1162e+09, 8.5436e+09, 7.8881e+09, 8.6683e+09],\n",
      "        [4.3255e+09, 4.1623e+09, 4.3815e+09, 4.0453e+09, 4.4454e+09],\n",
      "        [7.1948e+09, 6.9234e+09, 7.2880e+09, 6.7289e+09, 7.3943e+09],\n",
      "        [2.7138e+09, 2.6114e+09, 2.7489e+09, 2.5380e+09, 2.7891e+09]])\n",
      "tensor([[25,  7, 33, 15, 43],\n",
      "        [36, 20,  9, 54, 22],\n",
      "        [51, 19,  4, 24, 51],\n",
      "        [54, 55, 46, 52,  3]])\n",
      "tensor([[8.5256e+09, 8.2039e+09, 8.6360e+09, 7.9734e+09, 8.7620e+09],\n",
      "        [2.7425e+09, 2.6391e+09, 2.7781e+09, 2.5649e+09, 2.8186e+09],\n",
      "        [3.7386e+09, 3.5975e+09, 3.7870e+09, 3.4964e+09, 3.8422e+09],\n",
      "        [2.4056e+09, 2.3148e+09, 2.4367e+09, 2.2498e+09, 2.4723e+09]])\n",
      "tensor([[58, 53, 19, 57, 54],\n",
      "        [23,  6, 29,  7, 55],\n",
      "        [39, 24, 12, 41, 23],\n",
      "        [14, 23, 27, 44,  6]])\n",
      "tensor([[3.6118e+09, 3.4755e+09, 3.6586e+09, 3.3779e+09, 3.7120e+09],\n",
      "        [7.0243e+09, 6.7593e+09, 7.1153e+09, 6.5694e+09, 7.2191e+09],\n",
      "        [3.0613e+09, 2.9458e+09, 3.1010e+09, 2.8631e+09, 3.1462e+09],\n",
      "        [8.0528e+09, 7.7490e+09, 8.1571e+09, 7.5313e+09, 8.2761e+09]])\n",
      "tensor([[17, 36, 32, 59, 13],\n",
      "        [25, 50, 56, 54,  4],\n",
      "        [20, 14, 23, 29, 13],\n",
      "        [14, 16, 47, 21, 51]])\n",
      "tensor([[7.0684e+09, 6.8017e+09, 7.1599e+09, 6.6106e+09, 7.2644e+09],\n",
      "        [8.8408e+09, 8.5072e+09, 8.9553e+09, 8.2682e+09, 9.0859e+09],\n",
      "        [3.0709e+09, 2.9550e+09, 3.1106e+09, 2.8720e+09, 3.1560e+09],\n",
      "        [8.8855e+09, 8.5503e+09, 9.0006e+09, 8.3101e+09, 9.1319e+09]])\n",
      "tensor([[22,  7, 28,  2, 12],\n",
      "        [28, 15, 59,  1, 33],\n",
      "        [20, 13, 48, 10,  9],\n",
      "        [35, 16, 47,  2, 50]])\n",
      "tensor([[5.3809e+09, 5.1779e+09, 5.4506e+09, 5.0324e+09, 5.5301e+09],\n",
      "        [3.6345e+09, 3.4973e+09, 3.6815e+09, 3.3991e+09, 3.7353e+09],\n",
      "        [3.6775e+09, 3.5387e+09, 3.7251e+09, 3.4393e+09, 3.7795e+09],\n",
      "        [2.9542e+09, 2.8427e+09, 2.9924e+09, 2.7628e+09, 3.0361e+09]])\n",
      "tensor([[37, 14, 25, 37, 24],\n",
      "        [31, 24, 20,  1, 15],\n",
      "        [40, 19,  4, 58, 24],\n",
      "        [17, 57,  8, 55, 45]])\n",
      "tensor([[6.1529e+09, 5.9208e+09, 6.2326e+09, 5.7544e+09, 6.3235e+09],\n",
      "        [4.6713e+09, 4.4950e+09, 4.7318e+09, 4.3687e+09, 4.8008e+09],\n",
      "        [7.9838e+09, 7.6825e+09, 8.0871e+09, 7.4667e+09, 8.2051e+09],\n",
      "        [7.7224e+09, 7.4310e+09, 7.8224e+09, 7.2222e+09, 7.9365e+09]])\n",
      "tensor([[12, 22, 12, 24, 56],\n",
      "        [35, 29,  6, 31, 36],\n",
      "        [35,  1, 31, 38, 38],\n",
      "        [20, 54, 30, 24, 34]])\n",
      "tensor([[7.9321e+09, 7.6328e+09, 8.0348e+09, 7.4184e+09, 8.1520e+09],\n",
      "        [5.3604e+09, 5.1582e+09, 5.4298e+09, 5.0132e+09, 5.5090e+09],\n",
      "        [7.4225e+09, 7.1425e+09, 7.5186e+09, 6.9418e+09, 7.6283e+09],\n",
      "        [2.3555e+09, 2.2667e+09, 2.3860e+09, 2.2030e+09, 2.4209e+09]])\n",
      "tensor([[57, 37,  9, 31, 23],\n",
      "        [50, 31, 23, 59, 38],\n",
      "        [48, 56, 46, 51, 15],\n",
      "        [10,  0,  0,  2, 33]])\n",
      "tensor([[2.2730e+09, 2.1872e+09, 2.3024e+09, 2.1258e+09, 2.3360e+09],\n",
      "        [8.4797e+09, 8.1598e+09, 8.5895e+09, 7.9306e+09, 8.7149e+09],\n",
      "        [3.1812e+09, 3.0612e+09, 3.2224e+09, 2.9752e+09, 3.2694e+09],\n",
      "        [8.3380e+09, 8.0234e+09, 8.4460e+09, 7.7980e+09, 8.5692e+09]])\n",
      "tensor([[16, 25, 52, 49,  6],\n",
      "        [13, 27, 58, 31, 53],\n",
      "        [ 9,  6, 34, 11, 44],\n",
      "        [21, 42, 33, 47, 35]])\n",
      "tensor([[7.6487e+09, 7.3601e+09, 7.7477e+09, 7.1533e+09, 7.8608e+09],\n",
      "        [8.2425e+09, 7.9315e+09, 8.3492e+09, 7.7086e+09, 8.4710e+09],\n",
      "        [8.5420e+09, 8.2197e+09, 8.6526e+09, 7.9888e+09, 8.7788e+09],\n",
      "        [2.9837e+09, 2.8711e+09, 3.0223e+09, 2.7905e+09, 3.0664e+09]])\n",
      "tensor([[18,  9, 28, 45, 35],\n",
      "        [ 6, 20, 12, 36, 12],\n",
      "        [37, 11, 18, 27, 12],\n",
      "        [30,  0,  0, 32, 23]])\n",
      "tensor([[3.0999e+09, 2.9829e+09, 3.1400e+09, 2.8991e+09, 3.1859e+09],\n",
      "        [7.8377e+09, 7.5420e+09, 7.9392e+09, 7.3301e+09, 8.0550e+09],\n",
      "        [2.2601e+09, 2.1749e+09, 2.2894e+09, 2.1138e+09, 2.3228e+09],\n",
      "        [8.4873e+09, 8.1671e+09, 8.5972e+09, 7.9376e+09, 8.7227e+09]])\n",
      "tensor([[ 7, 57, 31,  5, 44],\n",
      "        [56, 26,  7, 16, 31],\n",
      "        [59, 30,  5, 13, 16],\n",
      "        [ 9, 14, 51, 40, 13]])\n",
      "tensor([[2.6473e+09, 2.5474e+09, 2.6815e+09, 2.4758e+09, 2.7207e+09],\n",
      "        [1.6191e+09, 1.5580e+09, 1.6400e+09, 1.5142e+09, 1.6640e+09],\n",
      "        [7.6451e+09, 7.3567e+09, 7.7441e+09, 7.1500e+09, 7.8571e+09],\n",
      "        [8.3640e+09, 8.0485e+09, 8.4723e+09, 7.8223e+09, 8.5960e+09]])\n",
      "tensor([[28, 28,  5, 58, 28],\n",
      "        [22, 33,  0,  2, 21],\n",
      "        [12,  9,  0, 37, 13],\n",
      "        [44,  6, 20,  2, 45]])\n",
      "tensor([[8.2736e+09, 7.9614e+09, 8.3807e+09, 7.7378e+09, 8.5030e+09],\n",
      "        [3.8960e+09, 3.7490e+09, 3.9465e+09, 3.6437e+09, 4.0041e+09],\n",
      "        [2.7330e+09, 2.6299e+09, 2.7684e+09, 2.5560e+09, 2.8087e+09],\n",
      "        [8.0977e+09, 7.7921e+09, 8.2025e+09, 7.5732e+09, 8.3222e+09]])\n",
      "tensor([[55, 48,  2,  9,  8],\n",
      "        [56, 30, 12,  7, 34],\n",
      "        [16, 46,  0,  1, 41],\n",
      "        [37, 15, 43,  3, 28]])\n",
      "tensor([[5.1928e+09, 4.9969e+09, 5.2600e+09, 4.8565e+09, 5.3368e+09],\n",
      "        [2.2551e+09, 2.1700e+09, 2.2843e+09, 2.1090e+09, 2.3176e+09],\n",
      "        [7.5220e+09, 7.2382e+09, 7.6194e+09, 7.0348e+09, 7.7306e+09],\n",
      "        [2.6700e+09, 2.5693e+09, 2.7046e+09, 2.4971e+09, 2.7440e+09]])\n",
      "tensor([[41, 48, 56, 34, 48],\n",
      "        [45, 15, 27, 35, 53],\n",
      "        [ 3, 51, 40, 22, 47],\n",
      "        [39, 55, 10, 53, 53]])\n",
      "tensor([[8.3366e+09, 8.0220e+09, 8.4445e+09, 7.7967e+09, 8.5677e+09],\n",
      "        [5.3365e+09, 5.1351e+09, 5.4056e+09, 4.9909e+09, 5.4845e+09],\n",
      "        [3.3621e+09, 3.2353e+09, 3.4056e+09, 3.1444e+09, 3.4553e+09],\n",
      "        [8.1852e+09, 7.8763e+09, 8.2912e+09, 7.6551e+09, 8.4121e+09]])\n",
      "tensor([[45, 28,  6, 51, 13],\n",
      "        [30, 58, 15, 18, 38],\n",
      "        [33, 20, 18, 21, 18],\n",
      "        [ 2, 57, 38, 37, 13]])\n",
      "tensor([[8.0548e+09, 7.7509e+09, 8.1591e+09, 7.5331e+09, 8.2781e+09],\n",
      "        [2.3338e+09, 2.2457e+09, 2.3640e+09, 2.1826e+09, 2.3985e+09],\n",
      "        [8.0327e+09, 7.7296e+09, 8.1367e+09, 7.5124e+09, 8.2554e+09],\n",
      "        [4.2500e+09, 4.0897e+09, 4.3050e+09, 3.9748e+09, 4.3679e+09]])\n",
      "tensor([[57, 42,  2, 10, 34],\n",
      "        [34, 44, 41, 11, 27],\n",
      "        [58, 32, 48, 54, 50],\n",
      "        [59, 30, 58, 42, 54]])\n",
      "tensor([[8.2122e+09, 7.9023e+09, 8.3185e+09, 7.6803e+09, 8.4399e+09],\n",
      "        [7.4212e+09, 7.1412e+09, 7.5173e+09, 6.9406e+09, 7.6270e+09],\n",
      "        [7.5575e+09, 7.2724e+09, 7.6554e+09, 7.0680e+09, 7.7671e+09],\n",
      "        [2.0488e+09, 1.9715e+09, 2.0753e+09, 1.9161e+09, 2.1056e+09]])\n",
      "tensor([[10,  9, 32, 42, 26],\n",
      "        [17, 51, 54, 13, 42],\n",
      "        [46, 13, 58, 19, 50],\n",
      "        [12, 22, 18, 51, 11]])\n",
      "tensor([[6.6281e+09, 6.3780e+09, 6.7139e+09, 6.1988e+09, 6.8119e+09],\n",
      "        [8.7247e+09, 8.3955e+09, 8.8376e+09, 8.1596e+09, 8.9666e+09],\n",
      "        [5.3111e+09, 5.1107e+09, 5.3798e+09, 4.9671e+09, 5.4583e+09],\n",
      "        [2.4884e+09, 2.3945e+09, 2.5206e+09, 2.3272e+09, 2.5574e+09]])\n",
      "tensor([[34, 30, 20, 32, 12],\n",
      "        [38, 26, 41, 31, 58],\n",
      "        [12, 36, 26, 54,  2],\n",
      "        [26, 21, 29, 25, 21]])\n",
      "tensor([[3.5290e+09, 3.3958e+09, 3.5747e+09, 3.3004e+09, 3.6268e+09],\n",
      "        [7.6947e+09, 7.4044e+09, 7.7943e+09, 7.1963e+09, 7.9080e+09],\n",
      "        [8.8720e+09, 8.5373e+09, 8.9869e+09, 8.2974e+09, 9.1180e+09],\n",
      "        [2.8322e+09, 2.7253e+09, 2.8688e+09, 2.6487e+09, 2.9107e+09]])\n",
      "tensor([[ 4, 13, 53, 29,  3],\n",
      "        [29, 11, 49,  9,  1],\n",
      "        [ 1, 51, 25, 36, 36],\n",
      "        [43, 56, 20, 26,  6]])\n",
      "tensor([[8.4665e+09, 8.1471e+09, 8.5761e+09, 7.9182e+09, 8.7013e+09],\n",
      "        [2.7986e+09, 2.6930e+09, 2.8348e+09, 2.6173e+09, 2.8762e+09],\n",
      "        [8.0469e+09, 7.7433e+09, 8.1511e+09, 7.5257e+09, 8.2700e+09],\n",
      "        [8.1675e+09, 7.8594e+09, 8.2733e+09, 7.6386e+09, 8.3940e+09]])\n",
      "tensor([[19, 57, 21, 28, 12],\n",
      "        [ 0,  5, 53,  7,  5],\n",
      "        [11, 27, 16, 54, 42],\n",
      "        [36,  1, 11, 17, 43]])\n",
      "tensor([[1.5985e+09, 1.5382e+09, 1.6192e+09, 1.4949e+09, 1.6428e+09],\n",
      "        [7.1387e+09, 6.8694e+09, 7.2311e+09, 6.6764e+09, 7.3366e+09],\n",
      "        [3.2102e+09, 3.0891e+09, 3.2518e+09, 3.0023e+09, 3.2992e+09],\n",
      "        [1.4655e+09, 1.4102e+09, 1.4845e+09, 1.3706e+09, 1.5062e+09]])\n",
      "tensor([[33, 42, 47, 48, 43],\n",
      "        [37, 56, 18, 11, 19],\n",
      "        [13, 21, 48, 27, 36],\n",
      "        [23, 50,  4, 11, 27]])\n",
      "tensor([[5.4957e+09, 5.2884e+09, 5.5669e+09, 5.1398e+09, 5.6481e+09],\n",
      "        [7.7732e+09, 7.4799e+09, 7.8738e+09, 7.2698e+09, 7.9887e+09],\n",
      "        [3.6948e+09, 3.5554e+09, 3.7427e+09, 3.4555e+09, 3.7973e+09],\n",
      "        [2.9081e+09, 2.7984e+09, 2.9457e+09, 2.7198e+09, 2.9887e+09]])\n",
      "tensor([[35,  0, 27, 36, 25],\n",
      "        [ 5, 53,  2, 12, 57],\n",
      "        [54, 19, 38,  8,  0],\n",
      "        [27, 24,  0, 31, 13]])\n",
      "tensor([[2.5817e+09, 2.4843e+09, 2.6151e+09, 2.4145e+09, 2.6533e+09],\n",
      "        [8.2872e+09, 7.9745e+09, 8.3945e+09, 7.7504e+09, 8.5169e+09],\n",
      "        [6.7323e+09, 6.4783e+09, 6.8195e+09, 6.2963e+09, 6.9190e+09],\n",
      "        [4.0552e+09, 3.9022e+09, 4.1077e+09, 3.7926e+09, 4.1677e+09]])\n",
      "tensor([[ 7, 39, 10, 18, 32],\n",
      "        [11, 34,  1, 21, 57],\n",
      "        [14, 20, 53, 39, 38],\n",
      "        [38, 37, 46,  2, 51]])\n",
      "tensor([[2.7170e+09, 2.6145e+09, 2.7522e+09, 2.5410e+09, 2.7923e+09],\n",
      "        [8.2584e+09, 7.9468e+09, 8.3653e+09, 7.7235e+09, 8.4874e+09],\n",
      "        [3.5951e+09, 3.4595e+09, 3.6416e+09, 3.3623e+09, 3.6948e+09],\n",
      "        [4.3433e+09, 4.1794e+09, 4.3995e+09, 4.0620e+09, 4.4637e+09]])\n",
      "tensor([[51, 11, 58, 33, 12],\n",
      "        [26, 38,  2, 14, 58],\n",
      "        [49, 39, 52, 57,  8],\n",
      "        [57, 53, 31, 35,  1]])\n",
      "tensor([[8.1692e+09, 7.8610e+09, 8.2750e+09, 7.6401e+09, 8.3957e+09],\n",
      "        [2.3971e+09, 2.3067e+09, 2.4281e+09, 2.2419e+09, 2.4636e+09],\n",
      "        [2.7613e+09, 2.6572e+09, 2.7971e+09, 2.5825e+09, 2.8379e+09],\n",
      "        [7.1778e+09, 6.9070e+09, 7.2708e+09, 6.7130e+09, 7.3769e+09]])\n",
      "tensor([[37, 34, 59, 52, 49],\n",
      "        [56, 29, 47,  8, 14],\n",
      "        [31, 14, 16, 59, 28],\n",
      "        [25, 31, 24, 38, 12]])\n",
      "tensor([[8.2810e+09, 7.9686e+09, 8.3883e+09, 7.7447e+09, 8.5107e+09],\n",
      "        [8.2925e+09, 7.9797e+09, 8.3999e+09, 7.7555e+09, 8.5225e+09],\n",
      "        [7.3759e+09, 7.0976e+09, 7.4714e+09, 6.8982e+09, 7.5804e+09],\n",
      "        [2.9500e+09, 2.8387e+09, 2.9882e+09, 2.7590e+09, 3.0318e+09]])\n",
      "tensor([[34, 52, 22, 41, 56],\n",
      "        [35, 13, 59, 48,  8],\n",
      "        [42, 50, 18,  2, 23],\n",
      "        [56,  1, 30, 10, 34]])\n",
      "tensor([[2.0034e+09, 1.9278e+09, 2.0294e+09, 1.8737e+09, 2.0590e+09],\n",
      "        [7.3446e+09, 7.0675e+09, 7.4397e+09, 6.8690e+09, 7.5483e+09],\n",
      "        [7.8979e+09, 7.5999e+09, 8.0001e+09, 7.3864e+09, 8.1169e+09],\n",
      "        [8.4437e+09, 8.1251e+09, 8.5530e+09, 7.8968e+09, 8.6778e+09]])\n",
      "tensor([[54, 31,  8, 17,  2],\n",
      "        [47, 41, 14, 25, 53],\n",
      "        [ 1, 34, 30, 47, 19],\n",
      "        [31, 16, 16, 33, 31]])\n",
      "tensor([[7.6534e+09, 7.3647e+09, 7.7525e+09, 7.1578e+09, 7.8657e+09],\n",
      "        [3.5577e+09, 3.4235e+09, 3.6038e+09, 3.3273e+09, 3.6563e+09],\n",
      "        [2.9852e+09, 2.8726e+09, 3.0239e+09, 2.7919e+09, 3.0680e+09],\n",
      "        [8.3642e+09, 8.0486e+09, 8.4725e+09, 7.8225e+09, 8.5961e+09]])\n",
      "tensor([[59, 19, 12, 55, 22],\n",
      "        [21, 57, 47, 36, 30],\n",
      "        [10, 10, 13,  7,  2],\n",
      "        [55, 49,  8, 10, 45]])\n",
      "tensor([[2.7657e+09, 2.6614e+09, 2.8015e+09, 2.5866e+09, 2.8424e+09],\n",
      "        [7.4660e+09, 7.1843e+09, 7.5626e+09, 6.9824e+09, 7.6730e+09],\n",
      "        [8.1853e+09, 7.8765e+09, 8.2913e+09, 7.6552e+09, 8.4123e+09],\n",
      "        [5.1918e+09, 4.9959e+09, 5.2590e+09, 4.8556e+09, 5.3358e+09]])\n",
      "tensor([[ 9, 14,  0, 26, 44],\n",
      "        [55, 31, 30, 22, 20],\n",
      "        [42, 46, 39, 33, 41],\n",
      "        [46, 15, 59, 41, 10]])\n",
      "tensor([[1.8259e+09, 1.7571e+09, 1.8496e+09, 1.7077e+09, 1.8766e+09],\n",
      "        [9.0718e+09, 8.7295e+09, 9.1892e+09, 8.4842e+09, 9.3233e+09],\n",
      "        [3.2425e+09, 3.1202e+09, 3.2845e+09, 3.0325e+09, 3.3324e+09],\n",
      "        [8.2548e+09, 7.9434e+09, 8.3617e+09, 7.7202e+09, 8.4837e+09]])\n",
      "tensor([[11, 10, 59, 50, 12],\n",
      "        [23, 44,  9, 15,  7],\n",
      "        [40, 25, 22, 57, 11],\n",
      "        [47, 15, 50, 57, 42]])\n",
      "tensor([[3.2956e+09, 3.1712e+09, 3.3382e+09, 3.0821e+09, 3.3869e+09],\n",
      "        [7.6692e+09, 7.3799e+09, 7.7685e+09, 7.1725e+09, 7.8819e+09],\n",
      "        [8.8769e+09, 8.5420e+09, 8.9918e+09, 8.3020e+09, 9.1230e+09],\n",
      "        [7.8761e+09, 7.5789e+09, 7.9781e+09, 7.3660e+09, 8.0945e+09]])\n",
      "tensor([[ 6, 26, 59, 29,  2],\n",
      "        [31, 21,  4, 20, 49],\n",
      "        [11, 51, 23,  3, 17],\n",
      "        [38, 47, 51,  9, 47]])\n",
      "tensor([[8.3860e+09, 8.0696e+09, 8.4945e+09, 7.8428e+09, 8.6185e+09],\n",
      "        [7.8104e+09, 7.5157e+09, 7.9116e+09, 7.3046e+09, 8.0270e+09],\n",
      "        [8.3360e+09, 8.0215e+09, 8.4439e+09, 7.7961e+09, 8.5671e+09]])\n",
      "tensor([[51,  1, 24, 50, 30],\n",
      "        [23, 20,  4, 27,  1],\n",
      "        [19, 47, 17, 22, 21]])\n",
      "Accuracy of the network on the 10000 test images: 0 %\n"
     ]
    }
   ],
   "source": [
    "tl = DataLoader(Captcha_Dataset.from_dir(file_locations[-128:-1]), \\\n",
    "    4, # Fetch 4 samples per batch\n",
    "    shuffle=True, num_workers=2)\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in tl:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = outputs.data # np.array([max(i) for i in outputs.data])\n",
    "        total += labels.size(0)\n",
    "        print(predicted)\n",
    "        print(labels)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a8928d6b6a07e0ed465d32f5bf5af9b2ab2f88849a9b2672ccc4b697fae7c51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
