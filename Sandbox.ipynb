{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identified 113062 images\n"
     ]
    }
   ],
   "source": [
    "file_locations = glob('./Captchas/*')\n",
    "captcha_names = [file.split('/')[-1].split('.')[0] for file in file_locations]\n",
    "print( f'identified {len(file_locations)} images' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 unique characters: ['Z', 'U', '7', 'I', 'K', '6', 'O', 'Y', 'J', 'A', 'R', '9', 'V', 'N', 'x', 'b', '3', 'm', 's', 'r', 'z', 'G', 'p', 'W', 'M', 'f', 'T', '5', 'h', 'e', 'E', 'k', 'y', 'c', 'X', '2', 'B', 't', 'w', 'j', '8', 'l', 'F', 'u', 'q', 'H', 'g', 'a', 'C', '1', '4', 'L', 'i', 'D', 'P', 'v', 'd', 'S', 'n', 'Q']\n"
     ]
    }
   ],
   "source": [
    "# Unique characters is global -- expect only 60 out\n",
    "unique_characters = [*set(char for name in captcha_names for char in name)]\n",
    "\n",
    "print( f'{len(unique_characters)} unique characters: {unique_characters}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Captcha_Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = data\n",
    "        self.y = labels\n",
    "        return \n",
    "\n",
    "    @classmethod\n",
    "    def import_image(cls, location:str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Import a single image.\n",
    "\n",
    "        Parameters: location (str) Location of image\n",
    "        Returns: (np.ndarray) Image dimensions = Captchas 40 x 150 x 3 RGB channels\n",
    "        \"\"\"\n",
    "        image = Image.open(location)\n",
    "        image.load()\n",
    "        #image.show()\n",
    "        data = np.asarray(image, dtype='float32')\n",
    "        return data\n",
    "    @classmethod\n",
    "    def stack_images(cls, file_locations:List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Stack imageset from directory.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (np.ndarray) len(file_locations) x image dimensions\n",
    "        \"\"\"\n",
    "        return np.array([cls.import_image(location) for location in file_locations ])\n",
    "    \n",
    "    @classmethod\n",
    "    def read_label_names(cls, file_locations:List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Simply extracts labels from filenames.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (List[str]) List of label names\n",
    "        \"\"\"\n",
    "        labels = [file.split('/')[-1].split('.')[0] for file in file_locations]\n",
    "        \n",
    "        return labels\n",
    "\n",
    "    @classmethod\n",
    "    def from_dir(cls, file_locations:List[str]):\n",
    "        \"\"\"\n",
    "        Instantiate from only a list of files.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (Captcha_Dataset) object\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            cls.stack_images(file_locations),\n",
    "            cls.read_label_names(file_locations)\n",
    "        )\n",
    "\n",
    "    def transform(self, image:np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Apply dataset transform.\"\"\"\n",
    "        return T.ToTensor()(image) # This is a hack for now.\n",
    "        # Not sure why, but this transforming doesn't work. It's weird. Idk.\n",
    "        # I originally tried using only PIL images and then resizing from there, but it didn't work.\n",
    "        # Tried now going from PIL --> ndarray --> PIL --> Tensor; also doesn't work. \n",
    "        # Bit lost.\n",
    "        # return  T.Compose([\n",
    "        #     T.ToPILImage(),\n",
    "        #     T.Resize([40, 150]),\n",
    "        #     T.ToTensor()\n",
    "        #     ])(image)\n",
    "\n",
    "    def encode_label(self, label:str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        label_array = []\n",
    "        for char in label:\n",
    "            node_array = [0]*len(unique_characters)\n",
    "            node_array[unique_characters.index(char)] += 1\n",
    "            label_array.append(node_array)\n",
    "        return np.array(label_array)\n",
    "\n",
    "    def __getitem__(self, index:int) -> Tuple[torch.Tensor, str]:\n",
    "        \"\"\"Select one sample. DataLoader accesses samples through this function.\"\"\"\n",
    "        return self.transform(self.X[index]), self.encode_label(self.y[index]), self.y[index]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Also needed for DataLoader.\"\"\"\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split dataset into 80:20 train/test of sizes 8000,2000.\n"
     ]
    }
   ],
   "source": [
    "# Split training/test data\n",
    "train_files, test_files = train_test_split(file_locations[0:10_000], test_size = .2)\n",
    "print(f'Split dataset into 80:20 train/test of sizes {len(train_files)},{len(test_files)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 40, 150, 3)\n",
      "Sample of images in format 40px x 150px x 3 RGB channels, of type <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# Load in training files.\n",
    "train = Captcha_Dataset.from_dir(train_files)#[0:128])\n",
    "\n",
    "print(f'{train.X.shape}\\nSample of images in format 40px x 150px x 3 RGB channels, of type {type(train.X[0][0][0][0])}')\n",
    "\n",
    "# Instantiate dataloader (the iterable that provides batches for gradient descent.)\n",
    "dl = DataLoader(train, \\\n",
    "    64, # Fetch 4 samples per batch\n",
    "    shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each batch has a dataset of shape torch.Size([64, 3, 40, 150]) and a corresponding set of torch.Size([64, 5, 60]) labels.\n"
     ]
    }
   ],
   "source": [
    "# Test out the iterable.\n",
    "dataiter = iter(dl)\n",
    "images, label_array, labels = next(dataiter)\n",
    "print(f'Each batch has a dataset of shape {images.shape} and a corresponding set of {label_array.shape} labels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, # 3 input channels\n",
    "            6, # 6 output channels\n",
    "            5, bias = False) # kernel of size 5x5\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, # kernel size of 2\n",
    "            2) # stride of 2\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 in / 16 out / 5x5 kernel\n",
    "\n",
    "        self.fc1 = nn.Linear(3808, # features in\n",
    "            1404) # features out\n",
    "\n",
    "        self.fc2 = nn.Linear(1404, 702)\n",
    "        self.fc3 = nn.Linear(702, \n",
    "                            len(unique_characters) * 5 # 300 output nodes: 5-chars 60 nodes\n",
    "                            , bias = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)#nn.Softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate all our stuff\n",
    "net = Net()\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "[1,     1] loss: 3.965e-03\n",
      "[1,    26] loss: 1.007e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pk/z60kgs_x3674rh85_xy_gt180000gn/T/ipykernel_13737/4210686782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Step optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/threeML/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/threeML/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(f'Starting epoch {epoch}')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i,(images, label_array, labels) in enumerate(dl,0):\n",
    "        \n",
    "        # Zero param grads\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        prediction = net(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(prediction.reshape(prediction.shape[0],5,60), label_array)\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        # Step optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # print stats\n",
    "        running_loss += loss.item()\n",
    "        if i%( len(train_files) // 64 // 10 ) == 0: print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / ( len(train_files) / 64 / 10 ):.3e}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './trained_net.pth'\n",
    "if False:\n",
    "    torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = DataLoader(Captcha_Dataset.from_dir(test_files), \\\n",
    "    4, # Fetch 4 samples per batch\n",
    "    shuffle=True, num_workers=2)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for (images, label_array, labels) in tl:\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        prediction = net(images)\n",
    "\n",
    "        pred_labels = []\n",
    "        for i,single_pred in enumerate(prediction.reshape(prediction.shape[0], 5, 60)):\n",
    "            captcha = ''\n",
    "            for char in single_pred:\n",
    "                outchar = unique_characters[np.argmax(char.detach())]\n",
    "                captcha += outchar\n",
    "            pred_labels.append(captcha)\n",
    "\n",
    "            if False: print('Predicted: %s Ground Truth: %s'%(captcha, labels[i]))\n",
    "            if captcha == labels[i]: correct+=1\n",
    "            total+=1\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a8928d6b6a07e0ed465d32f5bf5af9b2ab2f88849a9b2672ccc4b697fae7c51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
