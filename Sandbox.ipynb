{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identified 113062 images\n"
     ]
    }
   ],
   "source": [
    "file_locations = glob('./Captchas/*')\n",
    "captcha_names = [file.split('/')[-1].split('.')[0] for file in file_locations]\n",
    "print( f'identified {len(file_locations)} images' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_image = torchvision.io.read_image(file_locations[0])\n",
    "# torchvision.transforms.ToPILImage()(sample_image)\n",
    "# Odd, my installation doesn't have io for some reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Captcha_Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = data\n",
    "        self.y = labels\n",
    "\n",
    "    @classmethod\n",
    "    def import_image(cls, location:str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Import a single image.\n",
    "\n",
    "        Parameters: location (str) Location of image\n",
    "        Returns: (np.ndarray) Image dimensions = Captchas 40 x 150 x 3 RGB channels\n",
    "        \"\"\"\n",
    "        image = Image.open(location)\n",
    "        image.load()\n",
    "        #image.show()\n",
    "        data = np.asarray(image, dtype='float32')\n",
    "        return data\n",
    "    @classmethod\n",
    "    def stack_images(cls, file_locations:List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Stack imageset from directory.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (np.ndarray) len(file_locations) x image dimensions\n",
    "        \"\"\"\n",
    "        return np.array([cls.import_image(location) for location in file_locations ])\n",
    "    \n",
    "    @classmethod\n",
    "    def read_label_names(cls, file_locations:List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Simply extracts labels from filenames.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (List[str]) List of label names\n",
    "        \"\"\"\n",
    "        return [file.split('/')[-1].split('.')[0] for file in file_locations]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_dir(cls, file_locations:List[str]):\n",
    "        \"\"\"\n",
    "        Instantiate from only a list of files.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (Captcha_Dataset) object\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            cls.stack_images(file_locations),\n",
    "            cls.read_label_names(file_locations)\n",
    "        )\n",
    "\n",
    "    def transform(self, image:np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Dataset transform for loading.\"\"\"\n",
    "        return T.ToTensor()(image) # This is a hack for now.\n",
    "        # Not sure why, but this transforming doesn't work. It's weird. Idk.\n",
    "        # I originally tried using only PIL images and then resizing from there, but it didn't work.\n",
    "        # Tried now going from PIL --> ndarray --> PIL --> Tensor; also doesn't work. \n",
    "        # Bit lost.\n",
    "        # return  T.Compose([\n",
    "        #     T.ToPILImage(),\n",
    "        #     T.Resize([40, 150]),\n",
    "        #     T.ToTensor()\n",
    "        #     ])(image)\n",
    "\n",
    "    def __getitem__(self, index:int) -> Tuple[torch.Tensor, str]:\n",
    "        \"\"\"Select one sample. DataLoader accesses samples through this function.\"\"\"\n",
    "        return self.transform(self.X[index]), self.y[index]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Also needed for DataLoader.\"\"\"\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 40, 150, 3)\n",
      "Sample of 128 images in format 40px x 150px x 3 RGB channels, of type <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "sample = Captcha_Dataset.from_dir(file_locations[0:128])\n",
    "\n",
    "print(f'{sample.X.shape}\\nSample of 128 images in format 40px x 150px x 3 RGB channels, of type {type(sample.X[0][0][0][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(sample, \\\n",
    "    64, # Fetch 64 samples per batch\n",
    "    shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dl)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 40, 150])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a8928d6b6a07e0ed465d32f5bf5af9b2ab2f88849a9b2672ccc4b697fae7c51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
