{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identified 113062 images\n",
      "60 unique characters: {'i', '8', 'G', 'z', 'm', 'D', 'l', 'w', '2', 'R', 'k', 'p', 'X', 'J', 'n', 'C', 'd', 'V', 'j', 't', 's', '3', 'g', '6', '7', 'M', 'e', 'A', 'r', 'K', 'a', 'F', 'E', '4', 'U', 'T', 'N', 'W', '9', 'f', 'Q', 'O', 'c', 'I', '1', 'x', 'b', 'P', 'u', 'Y', 'H', 'h', 'q', 'B', '5', 'L', 'Z', 'S', 'y', 'v'}\n"
     ]
    }
   ],
   "source": [
    "file_locations = glob('./Captchas/*')\n",
    "captcha_names = [file.split('/')[-1].split('.')[0] for file in file_locations]\n",
    "unique_characters = set(char for name in captcha_names for char in name)\n",
    "print( f'identified {len(file_locations)} images' )\n",
    "print( f'{len(unique_characters)} unique characters: {unique_characters}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Captcha_Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = data\n",
    "        self.y = labels\n",
    "\n",
    "    @classmethod\n",
    "    def import_image(cls, location:str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Import a single image.\n",
    "\n",
    "        Parameters: location (str) Location of image\n",
    "        Returns: (np.ndarray) Image dimensions = Captchas 40 x 150 x 3 RGB channels\n",
    "        \"\"\"\n",
    "        image = Image.open(location)\n",
    "        image.load()\n",
    "        #image.show()\n",
    "        data = np.asarray(image, dtype='float32')\n",
    "        return data\n",
    "    @classmethod\n",
    "    def stack_images(cls, file_locations:List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Stack imageset from directory.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (np.ndarray) len(file_locations) x image dimensions\n",
    "        \"\"\"\n",
    "        return np.array([cls.import_image(location) for location in file_locations ])\n",
    "    \n",
    "    @classmethod\n",
    "    def read_label_names(cls, file_locations:List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Simply extracts labels from filenames.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (List[str]) List of label names\n",
    "        \"\"\"\n",
    "        return [file.split('/')[-1].split('.')[0] for file in file_locations]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_dir(cls, file_locations:List[str]):\n",
    "        \"\"\"\n",
    "        Instantiate from only a list of files.\n",
    "\n",
    "        Parameters: file_locations (List[str]) List of image locations\n",
    "        Returns: (Captcha_Dataset) object\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            cls.stack_images(file_locations),\n",
    "            cls.read_label_names(file_locations)\n",
    "        )\n",
    "\n",
    "    def transform(self, image:np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Dataset transform for loading.\"\"\"\n",
    "        return T.ToTensor()(image) # This is a hack for now.\n",
    "        # Not sure why, but this transforming doesn't work. It's weird. Idk.\n",
    "        # I originally tried using only PIL images and then resizing from there, but it didn't work.\n",
    "        # Tried now going from PIL --> ndarray --> PIL --> Tensor; also doesn't work. \n",
    "        # Bit lost.\n",
    "        # return  T.Compose([\n",
    "        #     T.ToPILImage(),\n",
    "        #     T.Resize([40, 150]),\n",
    "        #     T.ToTensor()\n",
    "        #     ])(image)\n",
    "\n",
    "    def __getitem__(self, index:int) -> Tuple[torch.Tensor, str]:\n",
    "        \"\"\"Select one sample. DataLoader accesses samples through this function.\"\"\"\n",
    "        return self.transform(self.X[index]), self.y[index]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Also needed for DataLoader.\"\"\"\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 40, 150, 3)\n",
      "Sample of first 128 images in format 40px x 150px x 3 RGB channels, of type <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "sample = Captcha_Dataset.from_dir(file_locations[0:128])\n",
    "\n",
    "print(f'{sample.X.shape}\\nSample of first 128 images in format 40px x 150px x 3 RGB channels, of type {type(sample.X[0][0][0][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(sample, \\\n",
    "    4, # Fetch 4 samples per batch\n",
    "    shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each batch has a dataset of shape torch.Size([4, 3, 40, 150]) and a corresponding set of 4 labels.\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(dl)\n",
    "images, labels = next(dataiter)\n",
    "print(f'Each batch has a dataset of shape {images.shape} and a corresponding set of {len(labels)} labels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, # 3 input channels\n",
    "            6, # 6 output channels\n",
    "            5) # kernel of size 5x5\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, # kernel size of 2\n",
    "            2) # stride of 2\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 in / 16 out / 5x5 kernel\n",
    "\n",
    "        self.fc1 = nn.Linear(3808, # features in\n",
    "            120) # features out\n",
    "\n",
    "        self.fc2 = nn.Linear(1404, 702)\n",
    "        self.fc3 = nn.Linear(702, 60)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0\n",
      "Read data.\n",
      "Grads zeroed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pk/z60kgs_x3674rh85_xy_gt180000gn/T/ipykernel_8486/2228664185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/threeML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/threeML/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/threeML/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    print(f'Starting epoch {epoch}')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dl, 0):\n",
    "        print(f'Iteration {i}')\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        print('Read data.')\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        print('Grads zeroed.')\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8046,  2.0783,  0.9522,  1.4867, -0.2791, -0.7392,  1.1491,  0.1646,\n",
       "         -0.0837, -1.6451],\n",
       "        [-0.1766,  5.0565,  2.4338,  0.9848, -3.1592, -0.4532,  1.3642,  0.5081,\n",
       "         -0.7839, -1.9731],\n",
       "        [ 1.5589,  4.8706,  1.6537,  1.1962, -2.1339,  0.2627, -0.2543,  1.7555,\n",
       "         -2.0709, -3.4647],\n",
       "        [ 0.9590,  3.3156,  1.2383,  1.6371, -1.6653,  1.2395,  1.1717,  1.2776,\n",
       "          0.0771, -3.3323]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threeML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a8928d6b6a07e0ed465d32f5bf5af9b2ab2f88849a9b2672ccc4b697fae7c51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
